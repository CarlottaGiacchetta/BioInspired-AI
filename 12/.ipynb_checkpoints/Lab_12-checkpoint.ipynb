{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "872dd06c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "## Goal.\n",
    "The goal of this lab is to study the application of quality-diversity algorithms, in particular the Multi-dimensional Archive of Phenotypic Elites (MAP-Elites) to various kinds of problems. We will also investigate the parametrization of the algorithm and its effect on the algorithmic performance.\n",
    "\n",
    "## Getting started. \n",
    "In this lab we will use the `qdpy`$^{[1]}$ library, as well as the `deap` library seen in the previous lab. All the exercises are based on the examples available in the library$^{[2]}$, with some modifications. Please note that the `qdpy` library contains many other quality-diversity algorithms, such as Novelty Search, and advanced variants of MAP-Elites. However, in this lab we will use for simplicity only the vanilla version of MAP-Elites.\n",
    "\n",
    "The basic version of MAP-Elites is shown in Algorithm 1. In the pseudo-code, $\\textbf{x}$ and $\\textbf{x}'$ are candidate solutions (i.e., $n$-dimensional vectors defined in the search space $\\textbf{D}$); $\\textbf{b}'$ is a *feature descriptor*, that is a location in a user-defined *discretized* feature space (which can be seen as a *grid* made of *bins*), corresponding to the candidate solution $\\textbf{x}'$, (i.e., an $N$-dimensional vector of user-defined features that characterize $\\textbf{x}'$, typically with $N<n$); $p'$ is the performance of the candidate solution $\\textbf{x}'$ (i.e., the scalar value returned by the objective function $f(\\textbf{x}')$; $\\mathcal{P}$ is a $<$feature descriptor, performance$>$ map (i.e., an associative table that stores the best performance associated to each feature descriptor encountered by the algorithm); $\\mathcal{X}$ is a $<$feature descriptor, solution$>$ map (i.e., an associative table that stores the best solution associated to each feature descriptor encountered by the algorithm); $\\mathcal{P}(\\textbf{b}')$ is the best performance associated to the feature descriptor $\\textbf{b}'$ (it can be empty); $\\mathcal{X}(\\textbf{b}')$ is the best solution associated to the feature descriptor $\\textbf{b}'$ (it can be empty).\n",
    "\n",
    "![alg1.png](img/img_12/alg1.png)\n",
    "\n",
    "Following the pseudo-code shown above, the algorithm first creates the two maps $\\mathcal{P}$ and $\\mathcal{X}$, which are initially empty. Then, a while loop is executed until a given stop criterion is not met (usually, on the maximum number of function evaluations). Each iteration of the loop evaluates a *batch* of solutions. In the first batch, a given number of solutions are randomly sampled, see the `randomSolution()` function, in the search space $\\textbf{D}$, which are used for initializing the two maps $\\mathcal{P}$ and $\\mathcal{X}$. Then, starting from the next iteration, solutions are first randomly selected from the current map $\\mathcal{X}$, through the randomSelection() operator, and then perturbed according to the `randomVariation()` operator. For each new solution $\\textbf{x}'$, the corresponding feature descriptor $\\textbf{b}'$ and performance $p'$ are then evaluated. At this point, the two maps $\\mathcal{P}$ and $\\mathcal{X}$ are updated: if the performance associated to $\\textbf{b}'$, $\\mathcal{P}(\\textbf{b}')$, is empty (which can happen if this is the first time that the algorithm generates a solution with that feature descriptor), or if it contains a value that is worse than the performance $p'$ of the newly generated solution (in the pseudo-code, we assume a minimization problem, therefore we check the condition $\\mathcal{P}(\\textbf{b}') > p'$), the new solution $\\textbf{x}'$ and its performance $p'$ are assigned to the elements of the maps corresponding to its feature descriptor $\\textbf{b}'$, namely $\\mathcal{P}(\\textbf{b}')$ and $\\mathcal{X}(\\textbf{b}')$. Once the loop terminates, the algorithm returns the two maps $\\mathcal{P}$ and $\\mathcal{X}$, which can be later analyzed for further inspection and post-processing.\n",
    "\n",
    "It can be immediately noted how simple the algorithm is. With reference to the pseudo-code, in order to apply MAP-Elites to a specific problem the following methods must be defined:\n",
    "\n",
    " - $\\textrm{randomSolution()}$: returns a randomly generated solution;\n",
    " - $\\textrm{randomSelection($\\mathcal{X}$)}$: randomly selects a solution from $\\mathcal{X}$;\n",
    " - $\\textrm{randomVariation($\\textbf{x}$)}$: returns a modified copy of $\\textbf{x}$;\n",
    " - $\\textrm{featureDescriptor}(\\textbf{x})$: maps a candidate solution $\\textbf{x}$ to its feature descriptor, $\\textbf{b}$;\n",
    " - $\\textrm{performance}(\\textbf{x})$: evaluates the objective function of the candidate solution $\\textbf{x}$.\n",
    "\n",
    "The first three methods are rather standard, i.e., they can be based on general-purpose operators typically used in EAs. However, it is possible to customize them according to the specific need. For instance, in the first two exercises of this lab we will use uniform random sampling and uniform random selection for the first two operators. For the variation operator, we will use the `RandomSearchMutPolyBounded` operator provided by `qdpy`, which essentially performs uniform random mutations with a saturation on the bounds of the search space. In the third exercise, we will use instead the typical operators of Genetic Programming.\n",
    "\n",
    "As for what concerns $\\textrm{featureDescriptor}(\\textbf{x})$ and $\\textrm{performance}(\\textbf{x})$, these are obviously problem-dependent: the first one, being dependent on how the user defines the features of interest and the corresponding feature space; the latter, being dependent on the specific objective function at hand. In the exercises, we will see different definitions of performances and descriptors.\n",
    "\n",
    "---\n",
    "[1]: A Quality-Diversity framework for Python 3.6+: https://gitlab.com/leo.cazenille/qdpy\n",
    "\n",
    "[2]: Examples avaiable at https://gitlab.com/leo.cazenille/qdpy/-/tree/master/examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf9861b",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "In this exercise, we will use MAP-Elites to _illuminate_ the feature space of a benchmark function that we have already used in some of the first labs, namely the Rastrigin function$^{[1]}$, which as you may remember is a highly multimodal problem.\n",
    "\n",
    "For simplicity, we will use as feature descriptor for MAP-Elites the first two variables of the problem. Note however that, in general, the features used in MAP-Elites can be any property (different from the fitness function) of the solutions to the problem at hand.\n",
    "\n",
    "To start the experiments, run the next cell. This will allow you to reproduce your results. At the end of the run, the script will generate a series of plots (see the figure below) in the `results/ex1/seed` directory, namely: \n",
    "\n",
    " - `activityGrid.pdf`: this map indicates, for each bin, how many times that bin has been updated (i.e., its elite has been replaced) during the evolutionary process;\n",
    " - `evals_contsize.pdf`: this trend indicates the cumulative number of bins filled during the evolutionary process; \n",
    " - `evals_fitnessmax0.pdf`: this trend is the usual fitness trend that we have seen in the previous labs (note: in this case the fitness has to be minimized);\n",
    " - `iterations_nbupdated.pdf`: this trend indicates how many bins are updated at each iteration of MAP-Elites;\n",
    " - `performancesGrid.pdf`: this is the final _illumination_ map that shows how the performance of the elites changes depending on the features at hand (brighter color indicates better results-note that the fitness is normalized in [0,1]).\n",
    "\n",
    "The main outputs of the experiments of the first exercise are an *activity grid* (see the figure below, left), and a *performance grid* (see the figure below, right). More plots are available in the exercise folder after the execution of the experiments.\n",
    "\n",
    "![ex1.png](img/img_12/ex1.png)\n",
    "\n",
    "Furthermore, the script will serialize the final version of the map handled by MAP-Elites in a pickle file named `final.p`, that can be deserialized and manipulated for further analysis.\n",
    "\n",
    " - What kind of considerations can you make regarding the fitness trend (Is the algorithm able to converge to a reasonably low fitness function? How quick is the convergence?), and the activity grid (For instance, are there regions of the feature space that are visited/updated more frequently than others?). What kind of illumination pattern do you observe? Do you see any trend/correlation between performance and features of the map?\n",
    "    \n",
    " - Try to change the parameters of the MAP-Elites algorithm, i.e.,: `NO_BINS`, `MAX_ITEMS_BIN`, `BUDGET`, `BATCH_SIZE`, which indicate, respectively, the number of bins (that is the same for both features), the maximum number of items stored in each bin of the grid, the total budget of the evolutionary process (number of function evaluations), and the batch size, i.e., how many solutions are evaluated at each iteration of MAP-Elites. Focus in particular on `NO_BINS`. What is the effect on the fitness trend and the performance map when you increase or decrease the number of bins?\n",
    "    \n",
    " - Try to change the problem dimension (`PROBLEM_DIM`) to a much larger value, for instance 10 (remember that Rastrigin is a scalable benchmark problem, meaning that it can be defined for any number of variables). Note that in any case the first two variables are taken as features for MAP-Elites. What kind of considerations can you make in this case regarding the illumination pattern and the other aspects (i.e., the fitness trend and the activity grid) of the results? Does illumination become more difficult (i.e., less bins are visited, with poorer performance)? Why?\n",
    "\n",
    "---\n",
    "\n",
    "[1]: Rastrigin function https://pythonhosted.org/inspyred/reference.html\\#inspyred.benchmarks.Rastrigin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c94008",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.utils_12.exercise_rastrigin import main\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "Edit this part to do the exercises\n",
    "\"\"\"\n",
    "\n",
    "# TODO: change these parameters\n",
    "NO_BINS=32          # default 32\n",
    "MAX_ITEMS_BIN=1     # default 1\n",
    "BUDGET=10000        # default 10000\n",
    "BATCH_SIZE=500      # default 500\n",
    "PROBLEM_DIM=3       # default 3\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "main(NO_BINS, MAX_ITEMS_BIN, BUDGET, BATCH_SIZE, PROBLEM_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c6e3ab",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "This exercise is similar to the previous one. The main difference is that in this case the objective function and feature descriptor are defined by a custom function, see `eval_fn`, that returns for each individual its fitness (`score`) and two features (`fit0` and `fit1`). Note that the fitness and features are based on trigonometric functions and are defined as scalable, i.e., they can be evaluated for any number of variables. \n",
    "\n",
    "To start the experiments, run the next cell. At the end of the run, the script will generate the same plots discussed in the previous exercise, as well as the pickle file containing the raw results, but it will save the results in the `results/ex2/seed` folder.\n",
    "\n",
    "\n",
    " - What kind of considerations can you make in this case regarding the fitness trend and illumination pattern?\n",
    " \n",
    " -  Also in this case, try to change `NO_BINS` and `PROBLEM_DIM`, and see if you can confirm the observations made in the previous experiment.\n",
    " \n",
    " - If you want, you could try to change the custom function definition in `eval_fn` and replicate the experiment with a different setting. What kind of results do you obtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0bd56d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.utils_12.exercise_custom_eval_fn import main\n",
    "import math\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "Edit this part to do the exercises\n",
    "\"\"\"\n",
    "\n",
    "# TODO: change these parameters\n",
    "NO_BINS=32          # default 32\n",
    "MAX_ITEMS_BIN=1     # default 1\n",
    "BUDGET=10000        # default 10000\n",
    "BATCH_SIZE=500      # default 500\n",
    "PROBLEM_DIM=3       # default 3\n",
    "\n",
    "# TODO: try to define a different objective function (score) and/or features (fit0, fit1)\n",
    "def eval_fn(ind):\n",
    "    \"\"\"An example evaluation function. It takes an individual as input, and returns the pair ``(fitness, features)``, where ``fitness`` and ``features`` are sequences of scores.\"\"\"\n",
    "    normalization = sum((x for x in ind))\n",
    "    k = 10.\n",
    "    score = 1. - sum(( math.cos(k * ind[i]) * math.exp(-(ind[i]*ind[i])/2.) for i in range(len(ind)))) / float(len(ind))\n",
    "    fit0 = sum((x * math.sin(abs(x) * 2. * math.pi) for x in ind)) / normalization\n",
    "    fit1 = sum((x * math.cos(abs(x) * 2. * math.pi) for x in ind)) / normalization\n",
    "    features = (fit0, fit1)\n",
    "    return (score,), features\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "main(eval_fn, NO_BINS, MAX_ITEMS_BIN, BUDGET, BATCH_SIZE, PROBLEM_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d612c540",
   "metadata": {},
   "source": [
    "# Instructions and questions\n",
    "\n",
    "Concisely note down your observations from the previous exercises (follow the bullet points) and think about the following questions. \n",
    "\n",
    " - Do you think there is a trade-off between quality and diversity, or one aspect is more important than the other? If so, which one, and in which circumstances?\n",
    " \n",
    " - In which kind of applications do you think that MAP-Elites (and quality-diversity algorithms in general) could be useful? Why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
